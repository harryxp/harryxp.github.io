<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Peachful Space - OpenGrok Indexer Code Analysis</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Peachful Space</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>OpenGrok Indexer Code Analysis</h1>

            <div class="info">
    Posted on February 10, 2015
    
</div>

<p>Just trying to understand OpenGrok internals. This analysis is loosely based on 0.12.4.</p>
<p>Essentially, the indexer does the follows:</p>
<ol style="list-style-type: decimal">
<li>build history cache</li>
<li>for each project, create a thread running <code>IndexDatabase.update()</code></li>
<li>each <code>IndexDatabase.update()</code> recursively processes files</li>
<li>for each file, it:
<ul>
<li>creates a Lucene <code>Document</code></li>
<li>adds a bunch of Lucene <code>Field</code>s</li>
<li>invokes <code>Analyzer.analyze()</code> on the file, which:
<ul>
<li>uses ctags to obtain symbols defined in the file</li>
<li>uses a JFlex-generated scanner (<code>&lt;LANG&gt;Xref.lex</code> -&gt; <code>org.opensolaris.opengrok.analysis.&lt;LANG&gt;.&lt;LANG&gt;Xref</code>) to generate html xref for the file</li>
<li>(since at this point it knows which symbols are definitions, it can generate different links for them vs. non-definitions)</li>
</ul></li>
<li>indexes the file using Lucene. The Analyzer provides a tokenizer generated by JFlex (<code>&lt;LANG&gt;SymbolTokenizer.lex</code> -&gt; <code>org.opensolaris.opengrok.analysis.&lt;LANG&gt;.&lt;LANG&gt;SymbolTokenizer</code>) that breaks the text into tokens</li>
</ul></li>
</ol>
<p>Typically, I run OpenGrok indexer with</p>
<pre><code>$ export OPENGROK_VERBOSE=true
$ export OPENGROK_PROGRESS=true
$ export OPENGROK_INSTANCE_BASE=~/og-instance-base
$ export OPENGROK_WEBAPP_CONTEXT=opengrok
$ ./OpenGrok index ~/og-instance-base/src</code></pre>
<p>The <code>OpenGrok</code> script has a trick - if you invoke it with environment variable <code>DO=echo</code>, it prints out the real java command. On my Debian the above expands to</p>
<pre><code>${JAVA}                                             /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java
    ${JAVA_OPTS}                                        -Xmx2048m
    ${PROPERTIES}                                       -Dorg.opensolaris.opengrok.history.git=/usr/bin/git
    ${LOGGER_PROPERTIES}                                -Djava.util.logging.config.file=/var/opengrok/logging.properties
    ${JAVA_DEBUG}
    -jar ${OPENGROK_JAR}                                -jar /var/opengrok/src/OpenGrok/dist/opengrok.jar
    ${IGNORE_PATTERNS}
    ${DERBY_OPTIONS}
    ${HISTORY_TAGS}
    ${REMOTE_REPOSITORIES}                              -r on
    ${SCAN_DEPTH}
    ${VERBOSE}                                          -v
    ${QUIET}
    ${PROGRESS}                                         -C
    ${EXUBERANT_CTAGS:+-c} ${EXUBERANT_CTAGS}           -c /usr/bin/ctags-exuberant
    ${CTAGS_OPTIONS_FILE:+-o}
    ${CTAGS_OPTIONS_FILE}
    ${OPENGROK_FLUSH_RAM_BUFFER_SIZE}
    ${SKIN}
    ${LEADING_WILDCARD}                                 -a on
    ${READ_XML_CONF}
    -W ${XML_CONFIGURATION}                             -W /var/opengrok/etc/configuration.xml
    ${SCAN_FOR_REPOSITORY}                              -S
    ${ENABLE_PROJECTS}                                  -P
    ${WEBAPP_CONFIG}                                    -U localhost:2424
    ${WEBAPP_CONTEXT}                                   -w opengrok
    -s ${SRC_ROOT}                                      -s /var/opengrok/src
    -d ${DATA_ROOT}&quot;                                    -d /var/opengrok/data
    &quot;${@}&quot;                                              -H</code></pre>
<p>Knowing what parameters are actually passed to java is extremely helpful down the road. Let’s follow the code flow.</p>
<pre><code>./OpenGrok index ~/og-instance-base/src
    LoadStandardEnvironment
    LoadInstanceConfiguration
        DefaultInstanceConfiguration
    ValidateConfiguration
    CreateRuntimeRequirements
    UpdateGeneratedData
        StdInvocation -H
            CommonInvocation -W ${XML_CONFIGURATION} ${SCAN_FOR_REPOSITORY} ${ENABLE_PROJECTS} ${WEBAPP_CONFIG} ${WEBAPP_CONTEXT} -s ${SRC_ROOT} -d ${DATA_ROOT}
                ${DO} ${JAVA} ${JAVA_OPTS} ${PROPERTIES} ${LOGGER_PROPERTIES} ${JAVA_DEBUG} -jar ${OPENGROK_JAR} ${IGNORE_PATTERNS} ${DERBY_OPTIONS} ${HISTORY_TAGS} ${REMOTE_REPOSITORIES} ${SCAN_DEPTH} ${VERBOSE} ${QUIET} ${PROGRESS} ${EXUBERANT_CTAGS:+-c} ${EXUBERANT_CTAGS} ${CTAGS_OPTIONS_FILE:+-o} ${CTAGS_OPTIONS_FILE} ${OPENGROK_FLUSH_RAM_BUFFER_SIZE} ${SKIN} ${LEADING_WILDCARD} ${READ_XML_CONF} &quot;${@}&quot;
    UpdateDescriptionCache                                                                              |
                                                                                                        |
                                                                          main class                    |
org/opensolaris/opengrok/index/Indexer.java         &lt;---------------------------------------------------+
    main()
        cmdOptions = new CommandLineOptions()
        Executor.registerErrorHandler()
        getopt = new GetOpt(argv, cmdOptions.getCommandString())
        getopt.parse()
        cfg = new Configuration()
        if there's a configuration file, try to load it first, then overwrite options with cmdline
        RepositoryFactory.getRepositoryClasses()
            classes that understand code repositories (Git etc.)
        RuntimeEnvironment env = RuntimeEnvironment.getInstance()
        env.setConfiguration(cfg, subFilesList)                         # subFilesList should be empty most of the time
        prepareIndexer()
            this method has many arguments, here's the corresponding command line options and typical values at this point
                env
                searchRepositories      -S, true
                addProjects             -P, true
                defaultProject          -p, null
                configFilename          -W, /var/opengrok/etc/configuration.xml
                refreshHistory          -H, true
                listFiles                   false
                createDict                  false
                subFiles                    empty ArrayList&lt;String&gt;
                repositories            -h, empty ArrayList&lt;String&gt;
                zapCache                -k, empty ArrayList&lt;String&gt;
                listRepos               -K, false
            if (searchRepositories ... )                                # -S, true
                HistoryGuru.addRepositories(env.getSourceRootPath())
                    call overloaded addRepositories()
                    RuntimeEnvironment.setRepositories()
                    invalidateRepositories()
                        refresh list of known repos (internal state of HistoryGuru)
                if (listRepoPaths || !zapCache.isEmpty))                # -K, false
                    not executed
            if (addProjects)                                            # -P, true
                env.getProjects() is empty so oldProjects is empty, too
                for each directory under env.getSourceRootFile()
                    new Project()
                put newly created Project objects into projects
            if (refreshHistory)                                         # -H, true
                HistoryGuru.createCache()
                    createCacheReal()
                        TODO
        if (runIndex || ... )                                           # runIndex is initialized to true
            progress = new DefaultIndexChangedListener()
            doIndexerExecution(update, noThreads, subFiles, progress)   # update is always true (stupid?)
                RuntimeEnvironment.register()
                    threadConfig.set(configuration)
                ExecutorService executor = Executors.newFixedThreadPool(noThreads)      # on my Debian noThreads = 8
                if (subFiles == null || subFiles.isEmpty())
                    if (update)
                        IndexDatabase.updateAll(executor, progress)
                            List&lt;IndexDatabase&gt; dbs = new ArrayList&lt;&gt;();
                            for each project in env.getProjects()
                                dbs.add(new IndexDatabase(project))                     # each IndexDatabase corresponds to one project
                                    new SimpleFSLockFactory()
                                    initialize()
                                        indexDir points to /var/opengrok/data/index/&lt;project&gt;
                                        indexDirectory = FSDirectory.open(indexDir, ...)        # Lucene directory
                                        analyzerGuru = new AnalyzerGuru()
                                        xrefDir points to /var/opengrok/data/xref/
                                        dirtyFile points to /var/opengrok/data/index/&lt;project&gt;/dirty
                            for each db in dbs                                                  # each iteration/thread handles one project
                                executor.submit(new Runnable() { run() { db.update() } })       # multithreading starts
                    else if (env.isOptimizeDatabase())                          |
                        ...                                                     |
                else                                                            |
                    ...                                                         |
                executor.shutdown()                                             |
                while (!executor.isTerminated())                                |
                    executor.awaitTermination()                                 |
                RuntimeEnvironment.destroyRenamedHistoryExecutor()              |
        sendToConfigHost(env, configHost)                                       |
                                                                                |
                                                    multithreading              |
IndexDatabase.update()              &lt;-------------------------------------------+
    String ctgs = RuntimeEnvironment.getCtags()
    ctags = new Ctags()
    analyzer = AnalyzerGuru.getAnalyzer()
        DEFAULT_ANALYZER_FACTORY.getAnalyzer()                              # use cache.  return a FileAnalyzer
    iwc = new IndexWriterConfig(SearchEngine.LUCENE_VERSION, analyzer)      # this analyzer is the default one for the writer.  we'll have many real ones later
    iwc.setOpenMode(OpenMode.CREATE_OR_APPEND)
    iwc.setRAMBufferSizeMB()
    writer = new IndexWriter(indexDirectory, iwc)
    writer.commit()                                     # to make sure index exists on disk
    directories.add(project.getPath())
    for each dir in directories
        sourceRoot points to /var/opengrok/src/&lt;project&gt;
        HistoryGuru.ensureHistoryCacheExists(sourceRoot)
        startuid corresponds to top project dir
        reader = DirectoryReader.open(indexDirectory)
        int numDocs = reader.numDocs()                                              # if project has been indexed before, numDocs &gt; 0
        if (numDocs &gt; 0)                                                            # skipped if project hasn't been indexed before
            uFields = MultiFields.getFields(reader)
            terms = uFields.terms(QueryBuilder.U)
        if (numDocs &gt; 0)                                                            # skipped if project hasn't been indexed before
            uidIter = terms.iterator(uidIter)                                       # uidIter is a TermsEnum
            TermsEnum.SeekStatus stat = uidIter.seekCeil(new BytesRef(startuid))
                if (...)
                    ...
        if (RuntimeEnvironment.isPrintProgress())                                   # -C, true
            file_cnt = indexDown(sourceRoot, dir, true, 0, 0)                       # count_only = true, get total file count for logging
        parent = dir                                                                # &quot;/&lt;project top dir&gt;&quot;
        indexDown(sourceRoot, parent, false, 0, file_cnt)
        |   for each file in sourceRoot.listFiles()
        |       if (accept(sourceRoot, file))                                       # check file is not a link to itself or one of its own parents, etc.
        |           String path = parent + '/' + file.getName()
        |           if (file.isDirectory())
        |               indexDown() recursively
        |           else
        |               if (RuntimeEnvironment.isPrintProgress() &amp;&amp; est_total &gt; 0 &amp;&amp; log.isLoggable(Level.INFO))
        |                   log.log(&quot;Progress: ...&quot;)
        |               if (uidIter != null)                                        # it's null if project hasn't been indexed before.  skip.
        |                   String uid = Util.path2uid(path, ...)
        |                   BytesRef buid = new BytesRef(uid)
        |                   while (...)
        |                       ...
        |                   if (...)
        |                       ...
        |               addFile(file, path)                                         # path is a string &quot;/&lt;project top dir&gt;/.../&lt;file name&gt;&quot;
        |                   in = BufferedInputStream(new FileInputStream(file))
        |                   fa = AnalyzerGuru.getAnalyzer(in, path)
        |                       FileAnalyzerFactory factory = find(in, file)
        |                           find(file)
        |                               use prefix then suffix to decide FileAnalyzerFactory
        |                           if find(file) fails, try find(in)
        |                               find(in) tries to use the first 8 bytes to decide
        |                               may return PlainAnalyzerFactory
        |                       return factory.getAnalyzer()                        # for PlainAnalyzerFactory this returns FileAnalyzer
        |                   for each listener in listeners                          # most of time it's only DefaultIndexChangedListener
        |                       listener.fileAdd(path, ...)
        |                   fa.setCtags(ctags)
        |                   fa.setProject(Project.getProject(path))
        |                   doc = new Document()
        |                   java.io.Writer xrefOut = getXrefWriter(fa, path)
        |                       g = fa.getFactory().getGenre()                      # Genre is an enum of { PLAIN(&quot;p&quot;), XREFABLE(&quot;x&quot;), IMAGE(&quot;i&quot;), DATA(&quot;d&quot;), HTML(&quot;h&quot;) }
        |                       if (xrefDir != null &amp;&amp; (g == Genre.PLAIN || g == Genre.XREFABLE))
        |                           make sure all the parent dirs of path exist
        |                           file = new File(xrefDir, path + &quot;.gz&quot;)
        |                           return new BufferedWriter(...)
        |                   analyzerGuru.populateDocument(doc, file, path, fa, xrefOut)
        |                       date = ...                                          # last modified time of file
        |                       doc.add(new Field(QueryBuilder.U, Util.path2uid(path, date), string_ft_stored_nanalyzed_norms))
        |                       doc.add(new Field(QueryBuilder.FULLPATH, file.getAbsolutePath(), string_ft_nstored_nanalyzed_norms))
        |                       HistoryReader hr = HistoryGuru.getHistoryReader(file)
        |                       if (hr != null)
        |                           doc.add(new TextField(QueryBuilder.HIST, hr))
        |                       doc.add(new Field(QueryBuilder.DATE, date, string_ft_stored_nanalyzed_norms))
        |                       doc.add(new TextField(QueryBuilder.PATH, path, Store.YES))
        |                       doc.add(new TextField(QueryBuilder.PROJECT, project.getPath(), Store.YES))
        |                       g = fa.getGenre()
        |                       if (g == Genre.PLAIN || g == Genre.XREFABLE || g == Genre.HTML) {
        |                           doc.add(new Field(QueryBuilder.T, g.typeName(), string_ft_stored_nanalyzed_norms))
        |                       fa.analyze(doc, StreamSource.fromFile(file), xrefOut);
        |                           although OpenGrok Analyzer inherits Lucene Analyzer, analyze() is only an OpenGrok thing
        |                           essentially it continues to doc.add() and writes xref
        |                           the real Lucene indexing happens with writer.addDocument() below
        |                           let's follow a particular analyzer  - JavaAnalyzer, which inherits analyze() from AbstractSourceCodeAnalyzer
        |                       String type = fa.getFileTypeName();                                                                 |
        |                       doc.add(new StringField(QueryBuilder.TYPE, type, Store.YES));                                       |
        |                   writer.addDocument(doc, fa)                                                                             |
        |                       At indexing, as a consequence of addDocument(doc), the Analyzer in effect for indexing is invoked for each indexed field of the added document.  See https://lucene.apache.org/core/4_10_3/core/org/apache/lucene/analysis/package-summary.html
        |                       AbstractSourceCodeAnalyzer.createComponents() uses the tokenizer defined in org.opensolaris.opengrok.analysis.&lt;LANG&gt; (generated by JFlex) against &quot;refs&quot; field
        |                       in other words, OpenGrok uses JFlex to generate scanners, as the implementation of tokenizers required by Lucene
        while(uidIter != null ...)                          # skipped during first-time indexing                                    |
            removeFile()                                                                                                            |
            ...                                                                                                                     |
        reader.close()                                                                                                              |
                                                                                                                                    |
                                                                                                                                    |
org/opensolaris/opengrok/analysis/plain/AbstractSourceCodeAnalyzer.java         &lt;---------------------------------------------------+
    analyze(doc, StreamSource.fromFile(file), xrefOut)
        PlainAnalyzer.analyze(doc, StreamSource.fromFile(file), xrefOut)
            doc.add(new TextField(QueryBuilder.FULL, getReader(src.getStream())))          # PlainAnalyzer.getReader() wraps a ExpandTabsReader around stream
            member Definitions defs = ctags.doCtags()
            |   initialize()
            |       build ctags command and options
            |       processBuilder = new ProcessBuilder(command)
            |       ctags = processBuilder.start()
            |       ctagsIn = new OutputStreamWriter(ctags.getOutputStream())                   # Java -&gt; ctags process
            |       ctagsOut = new BufferedReader(new InputStreamReader(ctags.getInpuStream())) # ctags process -&gt; Java
            |       use a thread to read standard error
            |   ctagsIn.write(file)
            |   ctagsIn.flush()
            |   ret = new Definitions()
            |   readTags(ret)
            |       do ... while (true)
            |           tagLine = ctagsOut.readLine()                           # a line of ctags output
            |           parse line
            |           addTag()
            |               Definitions.addTag()
            |                   tags.add(new Tag())
            |   return ret
            doc.add(new TextField(QueryBuilder.DEFS, new IteratorReader(defs.getSymbols())))
            doc.add(new TextField(QueryBuilder.REFS&quot;refs&quot;, getReader(src.getStream())))
            byte[] tags = defs.serialize()
            doc.add(new StoredField(QueryBuilder.TAGS, tags))
            addScopes()
                ...
            Reader in = getReader(src.getStream())
            writeXref(in, xrefOut)
                xref = newXref()                                                # often this is overriden.  here we look at JavaAnalyzer
                    new JavaXref()                                              # JavaXref is generated by JFlex
                xref.setDefs(defs)                                              # inherited from JFlexXref
                xref.project = project
                xref.write()                                                    # inherited from JFlexXref
                    writeSymbolTable()                                          # write get_sym_list() JavaScript function
                    setLineNumber()
                    startNewLine()
                        setLineNumber()
                        Util.readableLine()</code></pre>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
